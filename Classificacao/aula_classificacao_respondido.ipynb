{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de classificação com a base IRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo a base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "#Transformando em dataframe para facilitar a visualizacao\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuição dos dados da IRIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot dos dados em função de três atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number1 = 0\n",
    "feature_number2 = 1\n",
    "feature_number3 = 2\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "ax.scatter(iris_df.iloc[:, feature_number1], \n",
    "           iris_df.iloc[:, feature_number2], \n",
    "           iris_df.iloc[:, feature_number3], \n",
    "           c=iris_df.iloc[:, 4],\n",
    "           edgecolor='k', s=40)\n",
    "ax.set_title(\"Distribuição dos dados\")\n",
    "ax.set_xlabel(iris_df.columns[feature_number1])\n",
    "ax.set_ylabel(iris_df.columns[feature_number2])\n",
    "ax.set_zlabel(iris_df.columns[feature_number3])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot dos dados em função de dois atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_number1 = 0\n",
    "feature_number2 = 2\n",
    "\n",
    "plt.scatter(iris_df.iloc[:, feature_number1], \n",
    "            iris_df.iloc[:, feature_number2], \n",
    "            c=iris_df.iloc[:, 4],\n",
    "            edgecolor='k')\n",
    "\n",
    "plt.xlabel(iris.feature_names[feature_number1])\n",
    "plt.ylabel(iris.feature_names[feature_number2])\n",
    "\n",
    "plt.xlim(iris_df.iloc[:, feature_number1].min() - 0.5,\n",
    "         iris_df.iloc[:, feature_number1].max() + 0.5)\n",
    "\n",
    "plt.ylim(iris_df.iloc[:, feature_number2].min() - 0.5,\n",
    "         iris_df.iloc[:, feature_number2].max() + 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris_df.iloc[:,0:-1]\n",
    "Y = iris_df.iloc[:,-1]\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X,\n",
    "                                            Y,test_size=0.3,\n",
    "                                            shuffle=True, stratify=Y)\n",
    "#Quantidade de exemplos de cada classe no conjunto de treino\n",
    "print('Quantidade de exemplos em cada classe no conjunto de treino')\n",
    "Y_treino.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando uma MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier(hidden_layer_sizes = (100,100), max_iter = 1000)\n",
    "MLP.fit(X_treino,Y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo as predições e calculando a taxa de acerto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valores_preditos_teste = MLP.predict(X_teste)\n",
    "valores_preditos_treinamento = MLP.predict(X_treino)\n",
    "\n",
    "acuracia_teste = accuracy_score(Y_teste,\n",
    "                                valores_preditos_teste)\n",
    "\n",
    "acuracia_treinamento = accuracy_score(Y_treino,\n",
    "                                      valores_preditos_treinamento)\n",
    "\n",
    "kappa_teste = cohen_kappa_score(Y_teste,\n",
    "                                      valores_preditos_teste)\n",
    "\n",
    "kappa_treinamento = cohen_kappa_score(Y_treino,\n",
    "                                      valores_preditos_treinamento)\n",
    "\n",
    "matriz_confusao_teste = confusion_matrix(Y_teste,\n",
    "                                         valores_preditos_teste)\n",
    "\n",
    "print('Acuracia treino = ', acuracia_treinamento)\n",
    "print('Acuracia teste = ', acuracia_teste)\n",
    "print('Kappa treino = ', kappa_treinamento)\n",
    "print('Kappa teste = ', kappa_teste)\n",
    "print(matriz_confusao_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Treine uma random forest e calcule os resultados de Acurácia e Kappa para o conjunto de treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X_treino,Y_treino)\n",
    "\n",
    "valores_preditos_teste = randomForest.predict(X_teste)\n",
    "valores_preditos_treinamento = randomForest.predict(X_treino)\n",
    "\n",
    "acuracia_teste = accuracy_score(Y_teste,\n",
    "                                valores_preditos_teste)\n",
    "\n",
    "acuracia_treinamento = accuracy_score(Y_treino,\n",
    "                                      valores_preditos_treinamento)\n",
    "\n",
    "kappa_teste = cohen_kappa_score(Y_teste,\n",
    "                                      valores_preditos_teste)\n",
    "\n",
    "kappa_treinamento = cohen_kappa_score(Y_treino,\n",
    "                                      valores_preditos_treinamento)\n",
    "\n",
    "matriz_confusao_teste = confusion_matrix(Y_teste,\n",
    "                                         valores_preditos_teste)\n",
    "\n",
    "print('Acuracia treino = ', acuracia_treinamento)\n",
    "print('Acuracia teste = ', acuracia_teste)\n",
    "print('Kappa treino = ', kappa_treinamento)\n",
    "print('Kappa teste = ', kappa_teste)\n",
    "print(matriz_confusao_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de classificação com a base pima-indians-diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('dados/pima-indians-diabetes.csv')\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contando o número de exemplos de cada classe\n",
    "dados.iloc[:,-1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando a correlação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = dados.corr()\n",
    "\n",
    "# Gerando uma mascara para a parte de cima do triangulo\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Criando figura\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Desenhando o heatmap\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=0.35, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dados.iloc[:,0:-1]\n",
    "Y = dados.iloc[:,-1]\n",
    "X_treino, X_teste, Y_treino, Y_teste = train_test_split(X,Y,\n",
    "                                        test_size=0.3,stratify=Y)\n",
    "print('Quant de amostras de treino \\n', Y_treino.value_counts())\n",
    "print('\\nQuant de amostras de teste \\n', Y_teste.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando a MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier(hidden_layer_sizes=(1000,100),\n",
    "                    max_iter=1500,shuffle=True)\n",
    "\n",
    "MLP.fit(X_treino,Y_treino)\n",
    "Y_predito = MLP.predict(X_teste)\n",
    "matriz_confusao = confusion_matrix(Y_predito,Y_teste)\n",
    "acuracia = accuracy_score(Y_predito,Y_teste)\n",
    "kappa = cohen_kappa_score(Y_predito,Y_teste)\n",
    "\n",
    "print(matriz_confusao)\n",
    "print('Acurácia = ', acuracia)\n",
    "print('Kappa = ', kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atributos selecionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino_selecionados = X_treino.iloc[:,[0,1,5,6,7]]\n",
    "X_teste_selecionado = X_teste.iloc[:,[0,1,5,6,7]]\n",
    "\n",
    "MLP = MLPClassifier(hidden_layer_sizes=(1000,100),\n",
    "                    max_iter=1500,shuffle=True)\n",
    "\n",
    "MLP.fit(X_treino_selecionados,Y_treino)\n",
    "Y_predito = MLP.predict(X_teste_selecionado)\n",
    "matriz_confusao = confusion_matrix(Y_predito,Y_teste)\n",
    "acuracia = accuracy_score(Y_predito,Y_teste)\n",
    "kappa = cohen_kappa_score(Y_predito,Y_teste)\n",
    "\n",
    "print(matriz_confusao)\n",
    "print('Acurácia = ', acuracia)\n",
    "print('Kappa = ', kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler().fit(X_treino)\n",
    "X_treino_normalizado = scaler.transform(X_treino)\n",
    "X_teste_normalizado = scaler.transform(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Compare os resultados obtidos pela MLP treinada com os dados normalizados e não normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier(hidden_layer_sizes=(1000,100),\n",
    "                    max_iter=1500,shuffle=True)\n",
    "\n",
    "MLP.fit(X_treino_normalizado,Y_treino)\n",
    "Y_predito = MLP.predict(X_teste_normalizado)\n",
    "matriz_confusao_normalizada = confusion_matrix(Y_predito,Y_teste)\n",
    "acuracia_normalizada = accuracy_score(Y_predito,Y_teste)\n",
    "kappa_normalizada = cohen_kappa_score(Y_predito,Y_teste)\n",
    "\n",
    "print('DADOS NÃO NORMALIZADOS\\n',matriz_confusao)\n",
    "print('Acurácia = ', acuracia)\n",
    "print('Kappa = ', kappa)\n",
    "\n",
    "print('\\n\\nDADOS NORMALIZADOS\\n', matriz_confusao_normalizada)\n",
    "print('Acurácia = ', acuracia_normalizada)\n",
    "print('Kappa = ', kappa_normalizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanceando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Faça o balancemento dos dados de treino replicando os exemplos da classe minoritária. Em seguida, treine novamente a MLP com os dados normalizados e observe o que acontece com os resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_somente_classe_1 = X_treino.loc[Y_treino == 1,:]\n",
    "Y_somente_classe_1 = np.ones(X_somente_classe_1.shape[0])\n",
    "\n",
    "X_treino_balanceado = pd.concat([X_treino,\n",
    "                                 X_somente_classe_1])\n",
    "Y_treino_balanceado = pd.concat([Y_treino,\n",
    "                                 pd.DataFrame(Y_somente_classe_1,\n",
    "                                              dtype=np.int8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_balanceada = MLPClassifier(hidden_layer_sizes=(1000,100),\n",
    "                               max_iter=1500,shuffle=True)\n",
    "\n",
    "MLP_balanceada.fit(X_treino_balanceado,Y_treino_balanceado)\n",
    "Y_predito_balanceada = MLP_balanceada.predict(X_teste)\n",
    "matriz_confusao_balanceada = confusion_matrix(Y_predito_balanceada,\n",
    "                                              Y_teste)\n",
    "acuracia_balanceada = accuracy_score(Y_predito_balanceada,Y_teste)\n",
    "kappa_balanceada = cohen_kappa_score(Y_predito_balanceada,Y_teste)\n",
    "\n",
    "\n",
    "\n",
    "print('DADOS NÃO BALANCEADOS\\n',matriz_confusao)\n",
    "print('Acurácia = ', acuracia)\n",
    "print('Kappa = ', kappa)\n",
    "\n",
    "print('\\n\\nDADOS BALANCEADOS\\n', matriz_confusao_balanceada)\n",
    "print('Acurácia = ', acuracia_balanceada)\n",
    "print('Kappa = ', kappa_balanceada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_treino.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(Y_treino_balanceado == 0))\n",
    "print(np.sum(Y_treino_balanceado == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
